# ------------------------------------------------------------
# CISRE - Provider-agnostic model configuration
# ------------------------------------------------------------

# Global fallback for visual models
VLM_PROVIDER=google
VLM_MODEL_NAME=gemini-2.5-flash-lite
LLM_TEMPERATURE=0.0

# Optional overrides for ingestion (vision extraction)
# If you set these, they override VLM_* only for ingestion.
# Leave commented to inherit the global provider/model.
# INGEST_VLM_PROVIDER=google
# INGEST_VLM_MODEL_NAME=gemini-2.5-flash-lite
# Optional soft fallback (used only on technical parse failures)
# Set empty to disable escalation.
# INGEST_VLM_FALLBACK_MODEL_NAME=gemini-2.5-flash-lite
# INGEST_VLM_TEMPERATURE=0.0

# Optional overrides for chat (reasoning)
CHAT_LLM_PROVIDER=openai
CHAT_LLM_MODEL_NAME=gpt-4o-mini
CHAT_LLM_TEMPERATURE=0.1

# Provider credentials (set only the ones you use)
GEMINI_API_KEY=
# Alternatively supported for Google: GOOGLE_GENERATIVE_AI_API_KEY=
OPENAI_API_KEY=

# Storage target for batch file uploads
RAG_STORAGE_BUCKET=private_assets

# Visual routing cost guard (reduce latency/cost while stabilizing)
VISUAL_ROUTER_MAX_VISUAL_RATIO=0.35
VISUAL_ROUTER_MAX_VISUAL_PAGES=12

# Future local provider support (vLLM/Ollama)
LOCAL_MODEL_BASE_URL=http://localhost:8001/v1
LOCAL_MODEL_API_KEY=
