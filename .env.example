# ------------------------------------------------------------
# CISRE - Provider-agnostic model configuration
# ------------------------------------------------------------

# Global fallback for visual models
VLM_PROVIDER=google
VLM_MODEL_NAME=gemini-2.5-flash-lite
LLM_TEMPERATURE=0.0

# Optional overrides for ingestion (vision extraction)
# If you set these, they override VLM_* only for ingestion.
# Leave commented to inherit the global provider/model.
# INGEST_VLM_PROVIDER=google
# INGEST_VLM_MODEL_NAME=gemini-2.5-flash-lite
# Optional soft fallback (used only on technical parse failures)
# Set empty to disable escalation.
# INGEST_VLM_FALLBACK_MODEL_NAME=gemini-2.5-flash-lite
# INGEST_VLM_TEMPERATURE=0.0

# Optional overrides for chat (reasoning)
CHAT_LLM_PROVIDER=openai
CHAT_LLM_MODEL_NAME=gpt-4o-mini
CHAT_LLM_TEMPERATURE=0.1

# Provider credentials (set only the ones you use)
GEMINI_API_KEY=
# Alternatively supported for Google: GOOGLE_GENERATIVE_AI_API_KEY=
OPENAI_API_KEY=

# Storage target for batch file uploads
RAG_STORAGE_BUCKET=private_assets

# Runtime environment guardrails
# local: allows LOCAL or CLOUD embedding mode
# staging/production: LOCAL is blocked and forced to CLOUD
APP_ENV=local
RAG_SERVICE_SECRET=development-secret
JINA_MODE=CLOUD

# Backend data access
SUPABASE_URL=
SUPABASE_SERVICE_ROLE_KEY=

# Ingestion parser mode
# local: pymupdf4llm/fitz
# cloud: tries Jina Reader URL template, then falls back to local
INGEST_PARSER_MODE=local
# Optional template for cloud reader. Example:
# JINA_READER_URL_TEMPLATE=https://r.jina.ai/http://my-host/{path}
JINA_READER_URL_TEMPLATE=

# Reranking mode
# local: GravityReranker only
# jina: Jina reranker only
# hybrid: Jina + Gravity
RERANK_MODE=hybrid

# Authority classifier mode
# rules: rule-based classifier
# embedding_first: lightweight hashed-embedding classifier then rules fallback
AUTHORITY_CLASSIFIER_MODE=rules

# Visual routing cost guard (reduce latency/cost while stabilizing)
VISUAL_ROUTER_MAX_VISUAL_RATIO=0.35
VISUAL_ROUTER_MAX_VISUAL_PAGES=12

# Future local provider support (vLLM/Ollama)
LOCAL_MODEL_BASE_URL=http://localhost:8001/v1
LOCAL_MODEL_API_KEY=
